%
% File naaclhlt2012.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow} 
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Semantic Rescoring Framework of Spoken Language Understanding}

\author{Wencan Luo\\
	    Department of Computer Science\\
	    University of Pittsburgh\\
	    PA 15260, USA\\
	    {\tt wencan@cs.pitt.edu}
	  }

\date{September 4, 2013}

\begin{document}
\maketitle
\begin{abstract}
In this proposal, we are going to propose a model to resocre the N-Best given by automatic speech recognition (ASR).
However, the objective function is to maximizing the semantic meaning of candidate sentences but to minimize the Word Error Rate (WER).

\end{abstract}

\section{Introduction}
Let a computer understand what people say is a long term goal. However, ASR is not perfect in current stage. 
In fact, compared to manual transcriptions, ASR errors have a significant performance decrease in many NLP Tasks, such as Question Answering \cite{Turmo:2007}, Speaker Role Recognition \cite{Garg:2008}, Natural Language Understanding (NLU) \cite{Raymond:2007}, etc.

In dialogue systems, a NLU component produces a semantic representation that is appropriate for the dialogue task \cite{Jurafsky:2000}. In this paper, we use frames to represent the semantic knowledge, grounded in the theory of frame semantics \cite{Fillmore:1982}.

For Spoken Language Understanding (SLU), one challenge that has to deal with is the ASR errors, which is the focus in this paper.

\section{Related Work}
\subsection{NLU}

NLU is a well-study field and many techniques have been proposed to improve the performance. For example, Eun et al.\shortcite{Eun:2005} showed that combining several different classifiers promoted the performance of natural language understanding. 
 
Besides, both generative and discriminative models work pretty well for spoken language understanding \cite{Raymond:2007}, such as Finite State Transducers, CRF, SVM.
 
However, the majority of people work on human-transcribed text but rather than directly on speech recognition results. However, without ignoring the speech recognition errors, the performance is expected to decrease a lot. 

\subsection{Rescoring N-Best}
Rescoring N-Best has shown to be effective to decrease the Word Error Rate (WER) \cite{Zhang:2004,Zhou:2006}.

However, decreasing the WER is not our goal. Lower WER doesn't mean a better understanding, because, some words are more important than others in term of understanding. A better way to rescore the N-Best might be to directly focus on the meaning of the sentences. Quan et al. \shortcite{Quan:2005} proposed an extrinsic measurement to score N-Best with the ad-hoc task of Spoken Language Translation.

A recent work by Morbini et al. \shortcite{Morbini:2012} is most similar to us. They rescored the N-Best with the objective to increase the performance of NLU component by combining different ASR engines. However, it proposed a classification model to evaluate different ASR engines, but not the semantic meaning of the sentences.

\section{The Corpus}

There are several benchmark corpora that are commonly used by researchers to evaluate a SLU component.

\subsection{The ATIS}
The Air Travel Information System (ATIS) corpus \cite{Dahl:1994} has been used for the last decade to evaluate models of Automatic Speech Recognition and Understanding. It is made of single turns acquired with a Wizard of Oz (WOZ) approach, where users ask for fight information.

However, the ATIS is licensed by LDC, which costs \$1500.

\subsection{French MEDIAN}

The corpus MEDIA was collected within the French project MEDIA-EVALDA \cite{Bonneau-Maynard:2006} for development and evaluation of spoken understanding models and linguistic studies. The corpus is composed of 1,257 dialogs (from 250 different speakers) acquired with a Wizard of Oz (WOZ) approach in the context of hotel room reservations and tourist information.

However, the MEDIAN is licensed by ELRA, which costs 750 EURO.

\subsection{Polish LUNA}
The data for the Polish corpus has been collected at the Warsaw Transportation call center \cite{Marasek:2008}. This corpus covers the domain of transportation information like e.g.  transportation routes, itinerary, stops, or fare reductions.

The LUNA corpus is public available\footnote{http://zil.ipipan.waw.pl/LUNA}. It has 12,908 sentences for training and 3,005 for testing.

\subsection{The Bosch Corpus}
This data is collected by Amazon Mechanical Turk by the Bosch Research and Technology Center. %what does the task look like?
The turkers’ task is to response what they want to say when given a topic\footnote{A topic is described as a frame}. Totally, there are 3,364 distinct ways\footnote{A way to say something about a topic is a sentence.} for 11 topics and their frequencies follow the power law. The number of ways for each topic is shown in Table \ref{table:corpus_bosch} and the topics are grouped into four domains. These topics are chosen because they believe these are the most popular scenarios when people driving.
Among them, 1,870 of sentences are transcribed to speech by a native speaker. 700 of them are used as testing. Two speech recognition engines are used to recognize the speech to text: Google and Vocon. Both the two engines give N-Best outputs with confidence scores.

The topics and slots in this dataset are annotated by human. 

\begin{table}[!htb] 
\centering 
\input{corpus_bosch.tex} 
\caption{Number of Topics and Domains in the Bosch Corpus} 
\label{table:corpus_bosch} 
\end{table} 
 
\section{Methodology}
All the results below are based on experiments on the Bosch Corpus when I was an intern there, but I believe the results can be reproduced on other data too.
\subsection{Basic NLU}
For each frame, we have several slots associated with it. The NLU task is to identify slots given an utterance, which can be formed as a sequence labeling problem. For example, ``where is the best taco bell in palo alto" will be annotated as ``where/O is/O the/O best/B-psrh taco/B-ppn bell/I-ppn in/O palo/B-lcn alto/I-lcn". The BIO tags are used here, where `B' indicates the beginning of a slot; `I' means the inside of a slot; `O' means the ending of a slot. The slot name ``psrh" is short of ``property sorting rating high", ``ppn" is short of ``poiname" and ``lcn" is short for ``locationconstraint cityname".

We used CRF \cite{Lafferty:2001} to do the sequence labeling. The accuracies are shown in Table \ref{table:crf_basic}. A slot prediction is correct if and only if all the slots are extracted and the values are correct too.

\begin{table}[!htb] 
\centering 
\input{crf_basic.tex} 
\caption{Slot Prediction Accuracy on the Bosch Data, test on the manual transcription, Vocon ASR and Google ASR} 
\label{table:crf_basic} 
\end{table} 

\subsection{Combined N-Best}
As shown in Table \ref{table:crf_basic}, the accuracy on manual transcriptions is much higher than on ASR. It tells us that ASR is the major issue here.

Thus, it is reasonable to combine N-Best ASR to improve the performance.

We have tried three voting methods to combine the N-Best results.
 
{\bf Upper Bound}: 
If the correct one appears in any of the N-Best, it is a correct prediction. 
 
{\bf Majority Voting}: 
Use the majority as the prediction. Choose a random one if there are more than one of them. 
 
{\bf Weighted Voting}: 
``Majority Voting" does not consider the speech recognition ranking. The top ASR should get more weight. In this approach, each of the ASR gets the weight $(i+1)/i$, $i$ is the ASR rank. In this way, the top rank gets more weights.

The results are shown in Table \ref{table:slot_nbest_nodummy}. As we can see, there is still a big gap between the ``Upper Bound" and ``Weighted Voting", especially for the Google ASR. That's the motivation for this proposal.

The basic idea is to re-rank the N-Best with the objective function of maximizing the semantic meaning of sentences. Moreover, we are not trying to find the best recognition result which has the least word error rate, but to find a recognition that has a better semantics. Furthermore, we are not trying to find one sentence, but to select possible slot combination among the N-Best which maximizes the semantic meaning.

A possible solution might be Slot N-Gram. Train an n-gram model on the slot only, and select the candidate in the N-Best which has the lowest perplexity.

Another idea might be probabilistic frame-semantic parsing proposed in \cite{Das:2010}.

\begin{table*}[!htb] 
\centering 
\input{slot_nbest_nodummy.tex} 
\caption{Slot prediction accuracy with N-Best speech recognition result} 
\label{table:slot_nbest_nodummy} 
\end{table*} 

\section{Timeline}

\noindent \emph{Sep 09 - Sep 22}
\begin{itemize}
  \item survey the related work regarding frame-semantic model
  \item pick up a corpus as the test bank
\end{itemize}

\noindent \emph{Sep 23 -  Oct 20}
\begin{itemize}
  \item extract and format the data
  \item do Speech Recognition (SR)
  \item get the N-Best candidates on the chosen corpus
\end{itemize}

\noindent \emph{Oct 21 - Nov 9}
\begin{itemize}
  \item implement Slot N-Gram model to rescore the N-Best
\end{itemize}

\noindent \emph{Nov 10 - Dec 12}
\begin{itemize}
  \item try other models such as probabilistic frame-semantic
\end{itemize}

\section*{Acknowledgments}

Do not number the acknowledgment section.

\begin{thebibliography}{}

\bibitem[\protect\citename{Fillmore}1982]{Fillmore:1982}
C. J. Fillmore.
\newblock 1982.
\newblock {\em Frame semantics}.
\newblock In Linguistics in the Morning Calm, pages 111–137. Hanshin Publishing Co., Seoul, South Korea.

\bibitem[\protect\citename{Jurafsky and Martin}2000]{Jurafsky:2000}
D. Jurafsky and J. H. Martin.
\newblock 2000.
\newblock {\em Speech and Language Processing: An Introduction to Natural Language Processing}.
\newblock Computational Linguistics, and Speech Recognition, Prentice-Hall, Upper Saddle River, NJ, 2000.

\bibitem[\protect\citename{Turmo et al.}2007]{Turmo:2007}
J. Turmo, P. Comas, C. Ayache, D. Mostefa, S. Rosset, L. Lamel.
\newblock 2007.
\newblock {\em Overview of QAST 2007}.
\newblock In Working Notes of CLEF 2007 Workshop, Budapest, Hungary.

\bibitem[\protect\citename{Eun et al.}2005]{Eun:2005} 
J. Eun, M. Jeong, G. Geunbae Lee 
\newblock 2005. 
\newblock {\em A Multiple Classifier-based Concept-Spotting Approach for Robust Spoken Language Understanding.} 
\newblock Eurospeech, Lisbon, Portugal, pp. 3441-3444 

\bibitem[\protect\citename{Raymond and Riccardi}2007]{Raymond:2007} 
C. Raymond and G. Riccardi. 
\newblock 2007. 
\newblock {\em Generative and discriminative algorithms for spoken language understanding.} 
\newblock In Interspeech, pp. 1605–1608, Antwerp, Belgium, Aug. 2007. 

\bibitem[\protect\citename{Garg et al.}2008]{Garg:2008}
N. Garg, S. Favre, H. Salamin, D. Hakkani-Tur, and A. Vinciarelli. 
\newblock 2008. 
\newblock {\em Role recognition for meeting participants:an approach based on lexical information and social network analysis}.
\newblock Proceedings ACM International Conference on Multimedia, 693-696.

\bibitem[\protect\citename{Zhang and Rudnicky}2004]{Zhang:2004}
R. Zhang, and A.I. Rudnicky.
\newblock 2004. 
\newblock {\em Apply N-best list re-ranking to acoustic model combinations of Boosting Training}.
\newblock Proc. ICSLP, 2004. 

\bibitem[\protect\citename{Quan et al.}2005]{Quan:2005}
V. H. Quan, et al.
\newblock 2005. 
\newblock {\em Integrated n-best re-ranking for spoken language translation}.
\newblock in Proc. of Interspeech, Lisbon, Portugal.

\bibitem[\protect\citename{Zhou et al.}2006]{Zhou:2006}
Z. Zhou, J. Gao, F. K. Soong, and H. Meng.
\newblock 2006. 
\newblock {\em A comparative study of discriminative methods for reranking lvcsr n-best hypotheses in domain adaptation and generalization}.
\newblock in Proceedings of ICASSP, 2006, vol. 1, pp. 141-144. 

\bibitem[\protect\citename{Morbini et al.}2012]{Morbini:2012} 
F. Morbini, K. Audhkhasi, R. Artstein, M. Van Segbroeck, K. Sagae, P. Georgiou, D. R. Traum, S. Narayanan.
\newblock 2012. 
\newblock {\em A reranking approach for recognition and classification of speech input in conversational dialogue systems.} 
\newblock Spoken Language Technology Workshop (SLT), 2012 IEEE

\bibitem[\protect\citename{Bonneau-Maynard et al.}2006]{Bonneau-Maynard:2006} 
H\'{e}l\`{e}ne Bonneau-Maynard, Christelle Ayache, F. Bechet, A Denis, A Kuhn, Fabrice Lef\`{e}vre, D. Mostefa, M. Qugnard, S. Rosset, and J. Servan, S. Vilaneau. 
\newblock 2006. 
\newblock {\em Results of the french evalda-media evaluation campaign for literal understanding}.
\newblock In LREC, pages 2054-2059, Genoa, Italy, May 2006.

\bibitem[\protect\citename{Dahl et al.}1994]{Dahl:1994} 
D.A. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-Smith, D. Pallett, C. Pao, A. Rudnicky, and E. Shriberg.
\newblock 1994. 
\newblock {\em Expanding the scope of the atis task: the atis-3 corpus.}
\newblock In Proceedings of Human Language Technologies, page 4348, 1994.

\bibitem[\protect\citename{Marasek and Gubrynowicz}2008]{Marasek:2008} 
K. Marasek and R. Gubrynowicz. 
\newblock 2008. 
\newblock {\em Design and Data Collection for Spoken Polish Dialogs Database.} 
\newblock In Proc. of the Sixth Int. Conf. on Language Resources and Evaluation (LREC), Marrakech, Morocco, May 2008.

\bibitem[\protect\citename{Lafferty et al.}2001]{Lafferty:2001} 
J. Lafferty, A. McCallum, and F. Pereira.
\newblock 2001. 
\newblock {\em Conditional random fields: Probabilistic models for segmenting and labeling sequence data.} 
\newblock In Proc. of ICML, pp.282-289.

\bibitem[\protect\citename{Das et al.}2010]{Das:2010} 
D. Das, N. Schneider , D. Chen , N. A. Smith. 
\newblock 2010. 
\newblock {\em Probabilistic frame-semantic parsing.} 
\newblock Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, p.948-956, June 02-04, 2010, Los Angeles, California


\end{thebibliography}

\end{document}
